{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecasting BIG HiBM Returns using XGBoost\n",
    "\n",
    "_In this notebook, we forecast the monthly returns of the BIG HiBM portfolio using the XGBoost model. We begin by engineering features such as lagged returns and rolling statistics, then split the data into training (up to December 2015) and testing (from January 2016 onward) sets. Next, we tune the XGBoost model using a time-seriesâ€“aware cross-validation, evaluate its performance, and finally visualize the forecasts against the actual returns._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below creates lag features for the previous 1, 2, and 3 months as well as rolling mean and standard deviation over 3- and 6-month windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(series, lags=[1, 2, 3], windows=[3, 6]):\n",
    "    \"\"\"\n",
    "    Generate lagged features and rolling statistics for a time series.\n",
    "    \n",
    "    Parameters:\n",
    "        series (pd.Series): Time series data.\n",
    "        lags (list): List of lag periods to include.\n",
    "        windows (list): List of rolling window sizes (in months) for computing statistics.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the target variable and generated features.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({'y': series})\n",
    "    # Create lag features\n",
    "    for lag in lags:\n",
    "        df[f'lag_{lag}'] = df['y'].shift(lag)\n",
    "    # Create rolling features: rolling mean and std\n",
    "    for window in windows:\n",
    "        df[f'roll_mean_{window}'] = df['y'].rolling(window=window).mean()\n",
    "        df[f'roll_std_{window}'] = df['y'].rolling(window=window).std()\n",
    "    df.dropna(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date\n",
       "1990-07-01    0.0139\n",
       "1990-08-01   -0.1021\n",
       "1990-09-01   -0.1160\n",
       "1990-10-01    0.0874\n",
       "1990-11-01   -0.0345\n",
       "               ...  \n",
       "2024-08-01    0.0148\n",
       "2024-09-01    0.0069\n",
       "2024-10-01   -0.0071\n",
       "2024-11-01    0.0502\n",
       "2024-12-01   -0.0507\n",
       "Freq: MS, Name: BIG HiBM, Length: 414, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = r\"C:\\Users\\GIORDANO\\Desktop\\financial-time-series-forecasting\\data\\selected_portfolios.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path, parse_dates=True, index_col='date')\n",
    "df.index = pd.DatetimeIndex(df.index)\n",
    "df.index.freq = 'MS' \n",
    "big_returns = df['BIG HiBM'].dropna()\n",
    "big_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature DataFrame shape: (409, 8)\n",
      "                 y   lag_1   lag_2   lag_3  roll_mean_3  roll_std_3  \\\n",
      "date                                                                  \n",
      "1990-12-01  0.0028 -0.0345  0.0874 -0.1160     0.018567    0.062461   \n",
      "1991-01-01  0.0361  0.0028 -0.0345  0.0874     0.001467    0.035319   \n",
      "1991-02-01  0.1037  0.0361  0.0028 -0.0345     0.047533    0.051412   \n",
      "1991-03-01 -0.0338  0.1037  0.0361  0.0028     0.035333    0.068753   \n",
      "1991-04-01  0.0088 -0.0338  0.1037  0.0361     0.026233    0.070388   \n",
      "\n",
      "            roll_mean_6  roll_std_6  \n",
      "date                                 \n",
      "1990-12-01     -0.02475    0.076466  \n",
      "1991-01-01     -0.02105    0.079198  \n",
      "1991-02-01      0.01325    0.081604  \n",
      "1991-03-01      0.02695    0.059462  \n",
      "1991-04-01      0.01385    0.051622  \n"
     ]
    }
   ],
   "source": [
    "# Generating features using the create_features function\n",
    "feature_df = create_features(big_returns, lags=[1,2,3], windows=[3,6])\n",
    "print(\"Feature DataFrame shape:\", feature_df.shape)\n",
    "print(feature_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training (up to Dec 2015) and testing (from Jan 2016 onward)\n",
    "train_df = feature_df.loc[:'2015-12-31']\n",
    "test_df = feature_df.loc['2016-01-01':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (301, 7) (301,)\n",
      "Testing set: (108, 7) (108,)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into target and features\n",
    "X_train, y_train = train_df.drop(columns='y'), train_df['y']\n",
    "X_test, y_test = test_df.drop(columns='y'), test_df['y']\n",
    "\n",
    "print(\"Training set:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing set:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training using XGBoost \n",
    "\n",
    "We perform hyperparameter tuning using RandomizedSearchCV with a TimeSeriesSplit to respect the temporal order. This helps us select the best model configuration for forecasting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a parameter grid for XGBoost\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost regressor\n",
    "xgb_model = xgb.XGBRegressor(random_state=42)\n",
    "\n",
    "# Use TimeSeriesSplit for cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=20,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=tscv,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 7, 'learning_rate': 0.1, 'colsample_bytree': 1.0}\n"
     ]
    }
   ],
   "source": [
    "random_search.fit(X_train, y_train)\n",
    "print(\"Best parameters found:\", random_search.best_params_)\n",
    "best_xgb = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Forecast Performance:\n",
      "MAE: 0.0151\n",
      "RMSE: 0.0229\n"
     ]
    }
   ],
   "source": [
    "predictions = best_xgb.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "\n",
    "print(f\"XGBoost Forecast Performance:\\nMAE: {mae:.4f}\\nRMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test.index, y_test, label='Actual Returns', color='blue')\n",
    "plt.plot(y_test.index, predictions, label='XGBoost Predictions', color='red', linestyle='--')\n",
    "plt.title('XGBoost Forecast for BIG HiBM Returns')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Return (decimal)')\n",
    "plt.legend()\n",
    "plt.savefig('plots/xgboost_forecast.png')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
